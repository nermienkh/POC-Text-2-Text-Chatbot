{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nermienkh/POC-Text-2-Text-Chatbot/blob/main/TyDiQA_Subtask2_Arabic.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9x9Q57uSGude"
      },
      "source": [
        "#install needed dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XePDNP71g5H8",
        "outputId": "4526bcc2-95df-4522-9c6f-832a025f2e9f"
      },
      "source": [
        "!nvidia-smi"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jul  3 12:51:55 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   68C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "m-KYmncJY0Vs",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc60d793-aba3-40bd-aa8c-9888d47da9da"
      },
      "source": [
        "!pip install datasets\n",
        "!pip install transformers==4.2.16\n",
        "!pip install sentencepiece\n",
        "#only needed for AraBERTv1 and v2\n",
        "!pip install farasapy\n",
        "!pip install pyarabic\n",
        "!pip install fuzzysearch"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting datasets\n",
            "  Downloading datasets-2.13.1-py3-none-any.whl (486 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m486.2/486.2 kB\u001b[0m \u001b[31m20.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from datasets) (1.22.4)\n",
            "Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (9.0.0)\n",
            "Collecting dill<0.3.7,>=0.3.0 (from datasets)\n",
            "  Downloading dill-0.3.6-py3-none-any.whl (110 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m110.5/110.5 kB\u001b[0m \u001b[31m16.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets) (1.5.3)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (2.27.1)\n",
            "Requirement already satisfied: tqdm>=4.62.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (4.65.0)\n",
            "Collecting xxhash (from datasets)\n",
            "  Downloading xxhash-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (212 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m212.5/212.5 kB\u001b[0m \u001b[31m27.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting multiprocess (from datasets)\n",
            "  Downloading multiprocess-0.70.14-py310-none-any.whl (134 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m134.3/134.3 kB\u001b[0m \u001b[31m19.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: fsspec[http]>=2021.11.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (2023.6.0)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.8.4)\n",
            "Collecting huggingface-hub<1.0.0,>=0.11.0 (from datasets)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m27.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from datasets) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from datasets) (6.0)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4.0,>=2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (2.0.12)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.2)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.2)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (3.12.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0.0,>=0.11.0->datasets) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (2023.5.7)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.19.0->datasets) (3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets) (2022.7.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas->datasets) (1.16.0)\n",
            "Installing collected packages: xxhash, dill, multiprocess, huggingface-hub, datasets\n",
            "Successfully installed datasets-2.13.1 dill-0.3.6 huggingface-hub-0.15.1 multiprocess-0.70.14 xxhash-3.2.0\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement transformers==4.2.16 (from versions: 0.1, 2.0.0, 2.1.0, 2.1.1, 2.2.0, 2.2.1, 2.2.2, 2.3.0, 2.4.0, 2.4.1, 2.5.0, 2.5.1, 2.6.0, 2.7.0, 2.8.0, 2.9.0, 2.9.1, 2.10.0, 2.11.0, 3.0.0, 3.0.1, 3.0.2, 3.1.0, 3.2.0, 3.3.0, 3.3.1, 3.4.0, 3.5.0, 3.5.1, 4.0.0rc1, 4.0.0, 4.0.1, 4.1.0, 4.1.1, 4.2.0, 4.2.1, 4.2.2, 4.3.0rc1, 4.3.0, 4.3.1, 4.3.2, 4.3.3, 4.4.0, 4.4.1, 4.4.2, 4.5.0, 4.5.1, 4.6.0, 4.6.1, 4.7.0, 4.8.0, 4.8.1, 4.8.2, 4.9.0, 4.9.1, 4.9.2, 4.10.0, 4.10.1, 4.10.2, 4.10.3, 4.11.0, 4.11.1, 4.11.2, 4.11.3, 4.12.0, 4.12.1, 4.12.2, 4.12.3, 4.12.4, 4.12.5, 4.13.0, 4.14.0, 4.14.1, 4.15.0, 4.16.0, 4.16.1, 4.16.2, 4.17.0, 4.18.0, 4.19.0, 4.19.1, 4.19.2, 4.19.3, 4.19.4, 4.20.0, 4.20.1, 4.21.0, 4.21.1, 4.21.2, 4.21.3, 4.22.0, 4.22.1, 4.22.2, 4.23.0, 4.23.1, 4.24.0, 4.25.0, 4.25.1, 4.26.0, 4.26.1, 4.27.0, 4.27.1, 4.27.2, 4.27.3, 4.27.4, 4.28.0, 4.28.1, 4.29.0, 4.29.1, 4.29.2, 4.30.0, 4.30.1, 4.30.2)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for transformers==4.2.16\u001b[0m\u001b[31m\n",
            "\u001b[0mCollecting sentencepiece\n",
            "  Downloading sentencepiece-0.1.99-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: sentencepiece\n",
            "Successfully installed sentencepiece-0.1.99\n",
            "Collecting farasapy\n",
            "  Downloading farasapy-0.0.14-py3-none-any.whl (11 kB)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from farasapy) (2.27.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from farasapy) (4.65.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->farasapy) (3.4)\n",
            "Installing collected packages: farasapy\n",
            "Successfully installed farasapy-0.0.14\n",
            "Collecting pyarabic\n",
            "  Downloading PyArabic-0.6.15-py3-none-any.whl (126 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m126.4/126.4 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n",
            "Installing collected packages: pyarabic\n",
            "Successfully installed pyarabic-0.6.15\n",
            "Collecting fuzzysearch\n",
            "  Downloading fuzzysearch-0.7.3.tar.gz (112 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m112.7/112.7 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: attrs>=19.3 in /usr/local/lib/python3.10/dist-packages (from fuzzysearch) (23.1.0)\n",
            "Building wheels for collected packages: fuzzysearch\n",
            "  Building wheel for fuzzysearch (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fuzzysearch: filename=fuzzysearch-0.7.3-cp310-cp310-linux_x86_64.whl size=355231 sha256=5a5afc2ca91624a73eb2b2c680f2223c005c8623274e64790af0dafd10a84f07\n",
            "  Stored in directory: /root/.cache/pip/wheels/80/fb/bf/1c4f359d4b13bbc0e2cef8703d8a7c10dcd1e377496c19e6dc\n",
            "Successfully built fuzzysearch\n",
            "Installing collected packages: fuzzysearch\n",
            "Successfully installed fuzzysearch-0.7.3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPlY49JGG-MP"
      },
      "source": [
        "#Get QA code from the transformers library `transformers==4.3.3`"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vTeN0C-Md4xa",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "863d8c30-9a1a-410e-ab2e-bf04fa7200c2"
      },
      "source": [
        "!git clone https://github.com/aub-mind/arabert\n",
        "!cp arabert/examples/question-answering/utils_qa.py .\n",
        "!cp arabert/examples/question-answering/trainer_qa.py .\n",
        "!cp arabert/examples/question-answering/run_qa.py .\n",
        "!cp arabert/examples/question-answering/squad_preprocessing.py ."
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 24.57 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xlnh17yyHPX1"
      },
      "source": [
        "Download the TyDiQA data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YeKeqD79nNov",
        "outputId": "6df2842f-04b5-48bb-cd79-36c6aace16ee"
      },
      "source": [
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
        "!wget https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2023-07-02 19:27:08--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-train.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.136.128, 142.250.148.128, 209.85.200.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.136.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 58004076 (55M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-train.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-t 100%[===================>]  55.32M   129MB/s    in 0.4s    \n",
            "\n",
            "2023-07-02 19:27:08 (129 MB/s) - ‘tydiqa-goldp-v1.1-train.json’ saved [58004076/58004076]\n",
            "\n",
            "--2023-07-02 19:27:09--  https://storage.googleapis.com/tydiqa/v1.1/tydiqa-goldp-v1.1-dev.json\n",
            "Resolving storage.googleapis.com (storage.googleapis.com)... 142.250.136.128, 142.250.148.128, 209.85.200.128, ...\n",
            "Connecting to storage.googleapis.com (storage.googleapis.com)|142.250.136.128|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 5617409 (5.4M) [application/json]\n",
            "Saving to: ‘tydiqa-goldp-v1.1-dev.json’\n",
            "\n",
            "tydiqa-goldp-v1.1-d 100%[===================>]   5.36M  --.-KB/s    in 0.04s   \n",
            "\n",
            "2023-07-02 19:27:09 (123 MB/s) - ‘tydiqa-goldp-v1.1-dev.json’ saved [5617409/5617409]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1zY0nLLHJYM"
      },
      "source": [
        "#We need to preprocess the files before training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nyCG5O9wIutr"
      },
      "source": [
        "Preprocessing using fuzzy search is a very slow process. I advise that you save the preprocessed files somewhere for later use."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QH_wVusHT8Pw"
      },
      "source": [
        "model_name=\"aubmindlab/bert-base-arabertv02\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AyWDjnO053gz",
        "outputId": "f68b6371-ecfa-4d23-c564-f0f264e0276e"
      },
      "source": [
        "!rm -rf *-pre.json\n",
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-train.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-train-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-02 19:27:09.895668: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-02 19:27:10.894941: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-07-02 19:27:12.747405: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-02 19:27:13.294001: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-02 19:27:13.294286: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "aubmindlab/bert-base-arabertv02\n",
            " 56% 27713/49881 [00:20<00:24, 912.64it/s]WARNING:tensorflow:Could not find answer for question 'arabic-1804271180688859213-0' :\n",
            " 'الإسكندر الثالث المقدوني ، المعروف بأسماء عديدة أخرى أبرزها : الإسكندر الأكبر ، و < b data - parsoid = ' { \" dsr \" : [1849 , 1870 , 3 , 3] } ' > الإسكندر الكبير ، و < b data - parsoid = ' { \" dsr \" : [1873 , 1896 , 3 , 3] } ' > الإسكندر المقدوني ، و < b data - parsoid = ' { \" dsr \" : [1899 , 1924 , 3 , 3] } ' > الإسكندر ذو القرنين ( باليونانية : ؛ نقحرة : ) ، هو أحد ملوك مقدونيا الإغريق ، ومن أشهر القادة العسكريين والفاتحين عبر التاريخ . ولد الإسكندر في مدينة يلا قرابة سنة 356 ق . م ، وتتلمذ على يد الفيلسوف والعالم الشهير أرسطو حتى بلغ ربيعه السادس عشر . وبحلول عامه الثلاثين ، كان قد أسس إحدى أكبر وأعظم الإمبراطوريات التي عرفها العالم القديم ، والتي امتدت من سواحل البحر الأيوني غربا وصولا إلى سلسلة جبال الهيمالايا شرقا . يعد أحد أنجح القادة العسكريين في مسيرتهم ، إذ لم يحصل أن هزم في أي معركة خاضها على الإطلاق . [1]' \n",
            "vs.\n",
            " 'Ἀλέξανδρο'\n",
            "orig answer:\n",
            " 'Ἀλέξανδρο'\n",
            "==================\n",
            "100% 49881/49881 [00:27<00:00, 1817.74it/s]\n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 1 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zKVVljrMhI2",
        "outputId": "eb078180-ba77-440e-9e0e-238c18e91bc9"
      },
      "source": [
        "!python squad_preprocessing.py \\\n",
        "  --input_file \"tydiqa-goldp-v1.1-dev.json\" \\\n",
        "  --output_file \"tydiqa-goldp-v1.1-dev-pre.json\" \\\n",
        "  --model_name=$model_name \\\n",
        "  --filter_tydiqa=True"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2023-07-02 15:14:30.362632: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "2023-07-02 15:14:31.315925: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
            "2023-07-02 15:14:32.923552: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-02 15:14:32.982842: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "2023-07-02 15:14:32.983229: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
            "aubmindlab/bert-base-arabertv02\n",
            "100% 5077/5077 [00:01<00:00, 2724.69it/s] \n",
            "WARNING:tensorflow:Found 0 new answers: \n",
            "WARNING:tensorflow:Found 0 with no answers: \n",
            "WARNING:tensorflow:Found 0 with trunc answers: \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q_u20uFLMw7n"
      },
      "source": [
        "#Construct a custom dataset loader:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lLRe0YISeCpd",
        "outputId": "1c62a9d5-b3cb-4408-aec9-8b1c7730c6c6"
      },
      "source": [
        "#@title\n",
        "%%writefile artydiqa.py\n",
        "\"\"\"TODO(tydiqa): Add a description here.\"\"\"\n",
        "\n",
        "from __future__ import absolute_import, division, print_function\n",
        "\n",
        "import json\n",
        "import textwrap\n",
        "\n",
        "import datasets\n",
        "\n",
        "\n",
        "# TODO(tydiqa): BibTeX citation\n",
        "_CITATION = \"\"\"\\\n",
        "@article{tydiqa,\n",
        "title   = {TyDi QA: A Benchmark for Information-Seeking Question Answering in Typologically Diverse Languages},\n",
        "author  = {Jonathan H. Clark and Eunsol Choi and Michael Collins and Dan Garrette and Tom Kwiatkowski and Vitaly Nikolaev and Jennimaria Palomaki}\n",
        "year    = {2020},\n",
        "journal = {Transactions of the Association for Computational Linguistics}\n",
        "}\n",
        "\"\"\"\n",
        "\n",
        "# TODO(tydiqa):\n",
        "_DESCRIPTION = \"\"\"\\\n",
        "TyDi QA is a question answering dataset covering 11 typologically diverse languages with 204K question-answer pairs.\n",
        "The languages of TyDi QA are diverse with regard to their typology -- the set of linguistic features that each language\n",
        "expresses -- such that we expect models performing well on this set to generalize across a large number of the languages\n",
        "in the world. It contains language phenomena that would not be found in English-only corpora. To provide a realistic\n",
        "information-seeking task and avoid priming effects, questions are written by people who want to know the answer, but\n",
        "don’t know the answer yet, (unlike SQuAD and its descendents) and the data is collected directly in each language without\n",
        "the use of translation (unlike MLQA and XQuAD).\n",
        "\"\"\"\n",
        "\n",
        "# _URL = \"https://storage.googleapis.com/tydiqa/\"\n",
        "\n",
        "# _PRIMARY_URLS = {\n",
        "#     \"train\": _URL + \"v1.0/tydiqa-v1.0-train.jsonl.gz\",\n",
        "#     \"dev\": _URL + \"v1.0/tydiqa-v1.0-dev.jsonl.gz\",\n",
        "# }\n",
        "# _SECONDARY_URLS = {\n",
        "#     \"train\": _URL + \"v1.1/tydiqa-goldp-v1.1-train.json\",\n",
        "#     \"dev\": _URL + \"v1.1/tydiqa-goldp-v1.1-dev.json\",\n",
        "# }\n",
        "\n",
        "#use this for AraBERTv1 and V2\n",
        "_URL = \"https://storage.googleapis.com/tydiqa/\"\n",
        "_URL2 = \"/content/\"\n",
        "_PRIMARY_URLS = {\n",
        "    \"train\": _URL + \"v1.0/tydiqa-v1.0-train.jsonl.gz\",\n",
        "    \"dev\": _URL + \"v1.0/tydiqa-v1.0-dev.jsonl.gz\",\n",
        "}\n",
        "_SECONDARY_URLS = {\n",
        "    \"train\": _URL2 + \"tydiqa-goldp-v1.1-train-pre.json\",\n",
        "    \"dev\": _URL2 + \"tydiqa-goldp-v1.1-dev-pre.json\",\n",
        "}\n",
        "\n",
        "\n",
        "class TydiqaConfig(datasets.BuilderConfig):\n",
        "\n",
        "    \"\"\" BuilderConfig for Tydiqa\"\"\"\n",
        "\n",
        "    def __init__(self, **kwargs):\n",
        "        \"\"\"\n",
        "\n",
        "        Args:\n",
        "            **kwargs: keyword arguments forwarded to super.\n",
        "        \"\"\"\n",
        "        super(TydiqaConfig, self).__init__(version=datasets.Version(\"1.0.0\", \"\"), **kwargs)\n",
        "\n",
        "\n",
        "class Tydiqa(datasets.GeneratorBasedBuilder):\n",
        "    \"\"\"TODO(tydiqa): Short description of my dataset.\"\"\"\n",
        "\n",
        "    # TODO(tydiqa): Set up version.\n",
        "    VERSION = datasets.Version(\"0.1.0\")\n",
        "    BUILDER_CONFIGS = [\n",
        "        TydiqaConfig(\n",
        "            name=\"primary_task\",\n",
        "            description=textwrap.dedent(\n",
        "                \"\"\"\\\n",
        "          Passage selection task (SelectP): Given a list of the passages in the article, return either (a) the index of\n",
        "          the passage that answers the question or (b) NULL if no such passage exists.\n",
        "          Minimal answer span task (MinSpan): Given the full text of an article, return one of (a) the start and end\n",
        "          byte indices of the minimal span that completely answers the question; (b) YES or NO if the question requires\n",
        "          a yes/no answer and we can draw a conclusion from the passage; (c) NULL if it is not possible to produce a\n",
        "          minimal answer for this question.\"\"\"\n",
        "            ),\n",
        "        ),\n",
        "        TydiqaConfig(\n",
        "            name=\"secondary_task\",\n",
        "            description=textwrap.dedent(\n",
        "                \"\"\"Gold passage task (GoldP): Given a passage that is guaranteed to contain the\n",
        "          answer, predict the single contiguous span of characters that answers the question. This is more similar to\n",
        "          existing reading comprehension datasets (as opposed to the information-seeking task outlined above).\n",
        "          This task is constructed with two goals in mind: (1) more directly comparing with prior work and (2) providing\n",
        "          a simplified way for researchers to use TyDi QA by providing compatibility with existing code for SQuAD 1.1,\n",
        "          XQuAD, and MLQA. Toward these goals, the gold passage task differs from the primary task in several ways:\n",
        "          only the gold answer passage is provided rather than the entire Wikipedia article;\n",
        "          unanswerable questions have been discarded, similar to MLQA and XQuAD;\n",
        "          we evaluate with the SQuAD 1.1 metrics like XQuAD; and\n",
        "         Thai and Japanese are removed since the lack of whitespace breaks some tools.\n",
        "          \"\"\"\n",
        "            ),\n",
        "        ),\n",
        "    ]\n",
        "\n",
        "    def _info(self):\n",
        "        # TODO(tydiqa): Specifies the datasets.DatasetInfo object\n",
        "        if self.config.name == \"primary_task\":\n",
        "            return datasets.DatasetInfo(\n",
        "                # This is the description that will appear on the datasets page.\n",
        "                description=_DESCRIPTION,\n",
        "                # datasets.features.FeatureConnectors\n",
        "                features=datasets.Features(\n",
        "                    {\n",
        "                        \"passage_answer_candidates\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                \"plaintext_start_byte\": datasets.Value(\"int32\"),\n",
        "                                \"plaintext_end_byte\": datasets.Value(\"int32\"),\n",
        "                            }\n",
        "                        ),\n",
        "                        \"question_text\": datasets.Value(\"string\"),\n",
        "                        \"document_title\": datasets.Value(\"string\"),\n",
        "                        \"language\": datasets.Value(\"string\"),\n",
        "                        \"annotations\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                # 'annotation_id': datasets.Value('variant'),\n",
        "                                \"passage_answer_candidate_index\": datasets.Value(\"int32\"),\n",
        "                                \"minimal_answers_start_byte\": datasets.Value(\"int32\"),\n",
        "                                \"minimal_answers_end_byte\": datasets.Value(\"int32\"),\n",
        "                                \"yes_no_answer\": datasets.Value(\"string\"),\n",
        "                            }\n",
        "                        ),\n",
        "                        \"document_plaintext\": datasets.Value(\"string\"),\n",
        "                        # 'example_id': datasets.Value('variant'),\n",
        "                        \"document_url\": datasets.Value(\"string\")\n",
        "                        # These are the features of your dataset like images, labels ...\n",
        "                    }\n",
        "                ),\n",
        "                # If there's a common (input, target) tuple from the features,\n",
        "                # specify them here. They'll be used if as_supervised=True in\n",
        "                # builder.as_dataset.\n",
        "                supervised_keys=None,\n",
        "                # Homepage of the dataset for documentation\n",
        "                homepage=\"https://github.com/google-research-datasets/tydiqa\",\n",
        "                citation=_CITATION,\n",
        "            )\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            return datasets.DatasetInfo(\n",
        "                description=_DESCRIPTION,\n",
        "                features=datasets.Features(\n",
        "                    {\n",
        "                        \"id\": datasets.Value(\"string\"),\n",
        "                        \"title\": datasets.Value(\"string\"),\n",
        "                        \"context\": datasets.Value(\"string\"),\n",
        "                        \"question\": datasets.Value(\"string\"),\n",
        "                        \"answers\": datasets.features.Sequence(\n",
        "                            {\n",
        "                                \"text\": datasets.Value(\"string\"),\n",
        "                                \"answer_start\": datasets.Value(\"int32\"),\n",
        "                            }\n",
        "                        ),\n",
        "                    }\n",
        "                ),\n",
        "                # No default supervised_keys (as we have to pass both question\n",
        "                # and context as input).\n",
        "                supervised_keys=None,\n",
        "                homepage=\"https://github.com/google-research-datasets/tydiqa\",\n",
        "                citation=_CITATION,\n",
        "            )\n",
        "\n",
        "    def _split_generators(self, dl_manager):\n",
        "        \"\"\"Returns SplitGenerators.\"\"\"\n",
        "        # TODO(tydiqa): Downloads the data and defines the splits\n",
        "        # dl_manager is a datasets.download.DownloadManager that can be used to\n",
        "        # download and extract URLs\n",
        "        primary_downloaded = dl_manager.download_and_extract(_PRIMARY_URLS)\n",
        "        secondary_downloaded = dl_manager.download_and_extract(_SECONDARY_URLS)\n",
        "        if self.config.name == \"primary_task\":\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TRAIN,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"train\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.VALIDATION,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": primary_downloaded[\"dev\"]},\n",
        "                ),\n",
        "            ]\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            return [\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.TRAIN,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"train\"]},\n",
        "                ),\n",
        "                datasets.SplitGenerator(\n",
        "                    name=datasets.Split.VALIDATION,\n",
        "                    # These kwargs will be passed to _generate_examples\n",
        "                    gen_kwargs={\"filepath\": secondary_downloaded[\"dev\"]},\n",
        "                ),\n",
        "            ]\n",
        "\n",
        "    def _generate_examples(self, filepath):\n",
        "        \"\"\"Yields examples.\"\"\"\n",
        "        # TODO(tydiqa): Yields (key, example) tuples from the dataset\n",
        "        if self.config.name == \"primary_task\":\n",
        "            with open(filepath, encoding=\"utf-8\") as f:\n",
        "                for id_, row in enumerate(f):\n",
        "                    data = json.loads(row)\n",
        "                    passages = data[\"passage_answer_candidates\"]\n",
        "                    end_byte = [passage[\"plaintext_end_byte\"] for passage in passages]\n",
        "                    start_byte = [passage[\"plaintext_start_byte\"] for passage in passages]\n",
        "                    title = data[\"document_title\"]\n",
        "                    lang = data[\"language\"]\n",
        "                    question = data[\"question_text\"]\n",
        "                    annotations = data[\"annotations\"]\n",
        "                    # annot_ids = [annotation[\"annotation_id\"] for annotation in annotations]\n",
        "                    yes_no_answers = [annotation[\"yes_no_answer\"] for annotation in annotations]\n",
        "                    min_answers_end_byte = [\n",
        "                        annotation[\"minimal_answer\"][\"plaintext_end_byte\"] for annotation in annotations\n",
        "                    ]\n",
        "                    min_answers_start_byte = [\n",
        "                        annotation[\"minimal_answer\"][\"plaintext_start_byte\"] for annotation in annotations\n",
        "                    ]\n",
        "                    passage_cand_answers = [\n",
        "                        annotation[\"passage_answer\"][\"candidate_index\"] for annotation in annotations\n",
        "                    ]\n",
        "                    doc = data[\"document_plaintext\"]\n",
        "                    # example_id = data[\"example_id\"]\n",
        "                    url = data[\"document_url\"]\n",
        "                    yield id_, {\n",
        "                        \"passage_answer_candidates\": {\n",
        "                            \"plaintext_start_byte\": start_byte,\n",
        "                            \"plaintext_end_byte\": end_byte,\n",
        "                        },\n",
        "                        \"question_text\": question,\n",
        "                        \"document_title\": title,\n",
        "                        \"language\": lang,\n",
        "                        \"annotations\": {\n",
        "                            # 'annotation_id': annot_ids,\n",
        "                            \"passage_answer_candidate_index\": passage_cand_answers,\n",
        "                            \"minimal_answers_start_byte\": min_answers_start_byte,\n",
        "                            \"minimal_answers_end_byte\": min_answers_end_byte,\n",
        "                            \"yes_no_answer\": yes_no_answers,\n",
        "                        },\n",
        "                        \"document_plaintext\": doc,\n",
        "                        # 'example_id': example_id,\n",
        "                        \"document_url\": url,\n",
        "                    }\n",
        "        elif self.config.name == \"secondary_task\":\n",
        "            with open(filepath, encoding=\"utf-8\") as f:\n",
        "                data = json.load(f)\n",
        "                for article in data[\"data\"]:\n",
        "                    title = article.get(\"title\", \"\").strip()\n",
        "                    for paragraph in article[\"paragraphs\"]:\n",
        "                        context = paragraph[\"context\"].strip()\n",
        "                        for qa in paragraph[\"qas\"]:\n",
        "                            question = qa[\"question\"].strip()\n",
        "                            id_ = qa[\"id\"]\n",
        "                            if \"arabic\" not in id_:\n",
        "                              continue\n",
        "                            answer_starts = [answer[\"answer_start\"] for answer in qa[\"answers\"] if answer[\"answer_start\"] != -1]\n",
        "                            answers = [answer[\"text\"].strip() for answer in qa[\"answers\"] if answer[\"answer_start\"] != -1]\n",
        "                            if len(answers) == 0 or len(answer_starts)==0:\n",
        "                              print(\"question skipped\")\n",
        "                              continue\n",
        "                            assert len(answer_starts)==len(answers)\n",
        "                            # Features currently used are \"context\", \"question\", and \"answers\".\n",
        "                            # Others are extracted here for the ease of future expansions.\n",
        "                            yield id_, {\n",
        "                                \"title\": title,\n",
        "                                \"context\": context,\n",
        "                                \"question\": question,\n",
        "                                \"id\": id_,\n",
        "                                \"answers\": {\n",
        "                                    \"answer_start\": answer_starts,\n",
        "                                    \"text\": answers,\n",
        "                                },\n",
        "                            }"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing artydiqa.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KpsscdrUScsL"
      },
      "source": [
        "#Run Pytorch Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i3UHiqb_JfRb"
      },
      "source": [
        "#!python run_qa.py --help"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQyzBavn4kLt"
      },
      "source": [
        "model_name=\"aubmindlab/araelectra-base-discriminator\""
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QbD3-O-WfdlZ",
        "outputId": "0ecac515-c54c-464a-962a-7b79fdb073d8"
      },
      "source": [
        "!python run_qa.py \\\n",
        "  --model_name_or_path $model_name \\\n",
        "  --dataset_name artydiqa.py \\\n",
        "  --dataset_config_name \"secondary_task\" \\\n",
        "  --do_train \\\n",
        "  --do_eval \\\n",
        "  --per_device_train_batch_size 8 \\\n",
        "  --gradient_accumulation_steps 1 \\\n",
        "  --learning_rate 3e-5 \\\n",
        "  --num_train_epochs 4 \\\n",
        "  --max_seq_length 384 \\\n",
        "  --doc_stride 128 \\\n",
        "  --output_dir ./run \\\n",
        "  --n_best_size 20 \\\n",
        "  --evaluation_strategy epoch \\\n",
        "  --save_steps 20160 \\\n",
        "  --overwrite_output_dir \\\n",
        "  --seed 42 \\\n",
        "  --warmup_steps 500 \\\n",
        "  # --load_best_model_at_end \\\n",
        "  # --metric_for_best_model exact_match \\\n",
        "  # --greater_is_better True"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"/content/run_qa.py\", line 29, in <module>\n",
            "    import transformers\n",
            "ModuleNotFoundError: No module named 'transformers'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Run Inference using the ready model\n"
      ],
      "metadata": {
        "id": "27EjbHzlLQyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install transformers"
      ],
      "metadata": {
        "id": "qeZe6HmWLo5N",
        "outputId": "efac22b6-c283-4a9a-94f6-7bb80bbf7b5a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting transformers\n",
            "  Downloading transformers-4.30.2-py3-none-any.whl (7.2 MB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/7.2 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/7.2 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━\u001b[0m \u001b[32m5.1/7.2 MB\u001b[0m \u001b[31m75.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m90.0 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.2/7.2 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.2)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.22.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m88.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting safetensors>=0.3.1 (from transformers)\n",
            "  Downloading safetensors-0.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m86.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (4.6.3)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.16)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2023.5.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Installing collected packages: tokenizers, safetensors, transformers\n",
            "Successfully installed safetensors-0.3.1 tokenizers-0.13.3 transformers-4.30.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!git clone https://github.com/aub-mind/arabert\n",
        "!pip install pyarabic"
      ],
      "metadata": {
        "id": "xbNDRauwK2J4",
        "outputId": "a7b70961-110e-4113-8fdb-56b213f86cb0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'arabert'...\n",
            "remote: Enumerating objects: 600, done.\u001b[K\n",
            "remote: Counting objects: 100% (65/65), done.\u001b[K\n",
            "remote: Compressing objects: 100% (33/33), done.\u001b[K\n",
            "remote: Total 600 (delta 38), reused 45 (delta 30), pack-reused 535\u001b[K\n",
            "Receiving objects: 100% (600/600), 9.14 MiB | 13.59 MiB/s, done.\n",
            "Resolving deltas: 100% (339/339), done.\n",
            "Requirement already satisfied: pyarabic in /usr/local/lib/python3.10/dist-packages (0.6.15)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.10/dist-packages (from pyarabic) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from arabert.preprocess import ArabertPreprocessor\n",
        "from transformers import pipeline\n",
        "\n",
        "prep = ArabertPreprocessor(\"aubmindlab/araelectra-base-discriminator\") #or empty string it's the same\n",
        "qa_pipe_araelectra =pipeline(\"question-answering\",model=\"wissamantoun/araelectra-base-artydiqa\")\n",
        "\n"
      ],
      "metadata": {
        "id": "bICPu21rLZiM"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForQuestionAnswering, AutoTokenizer, pipeline\n",
        "\n",
        "model_name = \"deepset/xlm-roberta-large-squad2\"\n",
        "\n",
        "# a) Get predictions\n",
        "qa_pipe_xlm = pipeline('question-answering', model=model_name, tokenizer=model_name)"
      ],
      "metadata": {
        "id": "1pTYVnpx7cxw"
      },
      "execution_count": 50,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "context1 = \"\"\"\n",
        "تنظيم إجراءات المنافسات والمشتريات التي تقوم بها الجهات الحكومية ومنع تأثير المصالح الشخصية فيها، وذلك حماية للمال العام و تحقيق أقصى درجات الكفاية الاقتصادية للحصول على المشتريات الحكومية وتنفيذ مشروعاتها بأسعار تنافسية عادلة و تعزيز النزاهة والمنافسة، وتوفير معاملة عادلة للمتعهدين والمقاولين؛ تحقيقًا لمبدأ تكافؤ الفرص .و تحقيق الشفافية في جميع مراحل إجراءات المنافسات والمشتريات الحكومية .\n",
        "المادة الثانية، تتعامل الجهات الحكومية عند تنفيذ منافساتها وتوفير مشترياتها مع الأفراد والمؤسسات والشركات المرخص لهم بمزاولة العمل الذي تقع في نطاقه الأعمال طبقًا للأنظمة والقواعد المتبعة.\n",
        "المادة الثالثة ،مع مراعاة ما ورد في نظام الاستثمار الأجنبي ، يعطى جميع الأفراد والمؤسسات والشركات الراغبين في التعامل مع الحكومة ممن تتوافر فيهم الشروط التي تؤهلهم لهذا التعامل فرصًا متساوية ويعاملون على قدم المساواة .\n",
        "المادة الرابعة ، توفر للمتنافسين المعلومات الواضحة الكاملة والموحدة عن العمل المطلوب ، ويمكنون من الحصول على هذه المعلومات في وقت محدد ، كما توفر نسخ كافية من وثائق المنافسة لتلبية طلبات الراغبين في الحصول عليها .\n",
        "المادة الخامسة، تكون الأولوية في التعامل للمصنوعات والمنتجات والخدمات الوطنية وما يعامل معاملتها .\n",
        "المادة السادسة، تطرح جميع الأعمال والمشتريات الحكومية في منافسة عامة عدا ما يستثنى من المنافسة بموجب أحكام هذا النظام .\n",
        "المادة السابعة، يعلن عن جميع المنافسات الحكومية في الجريدة الرسمية وفي صحيفتين محليتين ، وبالوسائل الإعلانية الإلكترونية وفقًا لما تحدده اللائحة التنفيذية لهذا النظام ، ويجب أن يحدد في الإعلان عن المنافسة موعد تقديم العروض وفتح المظاريف ومكانهما . الأعمال أو المشاريع ذات الطبيعة الخاصة التي لا يتوفر لها متعهد أو المادة مقاول داخل المملكة يتم الإعلان عنها خارج المملكة بالإضافة إلى الإعلان عنها في الداخل وفقًا لما تضمنته الفقرة السابقة .\n",
        "المادة الثامنة لا يجوز قبول العروض والتعاقد بموجبها إلا طبقًا للشروط والمواصفات الموضوعة لها .\n",
        "المادة التاسعة ،يجب أن يتم الشراء وتنفيذ الأعمال والمشاريع بأسعار عادلة لا تزيد على الأسعار السائدة ، وتعد المنافسة الوسيلة العملية للوصول إلى ذلك وفق الأحكام الواردة في هذا النظام .\n",
        "المادة العاشرة، تقدم العروض في مظاريف مختومة في الموعد والمكان المحددين لقبولها . ولا يجوز قبول العروض التي تقدم أو تصل إلى الجهة الحكومية بعد انتهاء الموعد المحدد لتقديمها. ويجوز تقديم العروض وفتحها عن طريق الوسائل الإلكترونية وفقًا لما تحدده اللائحة التنفيذية لهذا النظام ، وتعلن الجهة الحكومية عن أسماء الشركات والمؤسسات التي تقدمت بعروضها .\n",
        "المادة الحادية عشرة، يقدم مع العرض ضمان ابتدائي يتراوح من (1%) إلى (2%) (من واحد إلى اثنين في المائة) من قيمته وفقًا لشروط المنافسة ، ولا يلزم تقديم هذا الضمان في الحالات التالية :  الشراء المباشر (إلا إذا كانت العروض مغلقة) .  تعاقدات الجهات الخاضعة لأحكام هذا النظام فيما بينها ، وفي التعاقد مع الجمعيات الخيرية والجمعيات ذات النفع العام ، بشرط تنفيذها الأعمال بنفسها .\n",
        "المادة الثانية عشرة،  تكون مدة سريان العروض في المنافسات العامة تسعين يومًا من التاريخ المحدد لفتح المظاريف ، فإن سحب مقدِّم العرض عرضه قبل انتهاء هذه المدة فلا يعاد له ضمانه الابتدائي .\n",
        " لا يجوز تمديد مدة سريان العرض والضمان الابتدائي إلا بموافقة مقدِّم العرض.\n",
        "المادة الثالثة عشرة ، يجب أن تحدد الأسعار الإجمالية وما يرد عليها من زيادة أو تخفيض في خطاب العرض الأصلي . ولا يعتد بأي تخفيض يقدم بوساطة خطاب مستقل حتى لو كان مرافقًا للعرض .\n",
        "ولا يجوز للمتنافسين في غير الحالات التي يجوز التفاوض فيها وفقًا لأحكام هذا النظام تعديل أسعار عروضهم بالزيادة أو التخفيض بعد تقديمها .\n",
        "المادة الرابعة عشرة ،تكوّن لجنة أو أكثر لدى الجهة الحكومية لفتح المظاريف لا يقل عدد أعضائها عن ثلاثة إضافة إلى رئيسها الذي لا تقل مرتبته عن العاشرة أو ما يعادلها . وينص في التكوّين على عضو احتياطي يكمل النصاب إن غاب أحد الأعضاء . ويعاد تكوّين اللجنة كل ثلاث سنوات .\n",
        "المادة الخامسة عشرة ،تفتح المظاريف بحضور جميع أعضاء لجنة فتح المظاريف في الموعد المحدد لذلك . وتعلن على من حضر من المتنافسين أو مندوبيهم الأسعار الواردة في العروض ، ويجب على هذه اللجنة إحالة محضرها وأوراق المنافسة إلى لجنة فحص العروض خلال سبعة أيام من تاريخ فتح المظاريف .\n",
        "المادة السادسة عشرة ،  تكوّن في الجهة الحكومية لجنة أو أكثر لفحص العروض تتكون من ثلاثة أعضاء على الأقل إضافة إلى رئيسها الذي لا تقل مرتبته عن الثالثة عشرة أو ما يعادلها ، على أن يكون من بينهم المراقب المالي ومن هو مؤهل تأهيلًا نظاميًّا . ويُنص في التكوّين على عضو احتياطي يكمل النصاب إن غاب أحد الأعضاء ، وتتولى هذه اللجنة تقديم توصياتها في الترسية على أفضل العروض وفقًا لأحكام هذا النظام ولائحته التنفيذية ، ولها أن تستعين في تقديم توصياتها بتقرير من فنيين متخصصين .\n",
        " يعاد تكوين اللجنة كل سنة .\n",
        "المادة السابعة عشرة ،لا يجوز الجمع بين رئاسة لجنة فحص العروض وصلاحية البت في المنافسة ، كما لا يجوز الجمع بين رئاسة لجنة فتح المظاريف ورئاسة لجنة فحص العروض أو العضوية فيهما .\n",
        "المادة الثامنة عشرة ، يجوز أن يرأس لجنة فحص العروض موظف لا تقل مرتبته عن العاشرة أو ما يعادلها ، إذا كانت مُشكَّلة في غير مقر الجهة الرئيس .\n",
        "المادة التاسعة عشرة ، تتخذ اللجنة توصياتها بحضور كامل أعضائها ، وتدون هذه التوصيات في محضر ، ويوضح الرأي المخالف إن وجد ، وحجة كلا الرأيين ، ليعرض على صاحب الصلاحية للبت في الترسية بما يتفق مع أحكام هذا النظام .\n",
        "المادة العشرون ، يجب على الجهة الحكومية البت في العروض واعتماد الترسية خلال المدة المحددة لسريان العروض المشار إليها في المادة الثانية عشرة ، وبعد انتهاء هذه المدة تتخذ الإجراءات اللازمة لإعادة الضمانات الابتدائية لأصحابها .\n",
        "المادة الحادية والعشرون ،  يجوز للجنة فحص العروض التفاوض مع صاحب أقل عرض مطابق للشروط والمواصفات ثم مع من يليه من المتنافسين في الحالتين التاليتين\n",
        "إذا ارتفعت العروض عن أسعار السوق بشكل ظاهر تحدد اللجنة مبلغ التخفيض بما يتفق مع أسعار السوق ، وتطلب كتابيًّا من صاحب العرض الأقل تخفيض سعره . فإن امتنع أو لم يصل بسعره إلى المبلغ المحدد ، تتفاوض مع العرض الذي يليه وهكذا . فإن لم يتم التوصل إلى السعر المحدد تلغى المنافسة ، ويعاد طرحها من جديد .  إذا زادت قيمة العروض على المبالغ المعتمدة للمشروع ، يجوز للجهة الحكومية إلغاء بعض البنود أو تخفيضها للوصول إلى المبالغ المعتمدة بشرط أن لا يؤثر ذلك على الانتفاع بالمشروع أو ترتيب العروض وإلا تُلغى المنافسة .\n",
        "المادة الثانية والعشرون ، لا يجوز استبعاد أي عرض بحجة تدني أسعاره إلا إذا قل بنسبة (35%) خمسة وثلاثين في المائة فأكثر عن تقديرات الجهة الحكومية والأسعار السائدة ، ويجوز للجنة فحص العروض بعد مناقشة صاحب العرض وإجراء التحليل المالي والفني ووصولها إلى قناعة بمقدرة صاحب العرض على تنفيذ العقد التوصية بعدم استبعاد العرض .\n",
        "المادة الثالثة والعشرون ، يجوز للجنة فحص العروض التوصية باستبعاد أي عرض من العروض من المنافسة حتى لو كان أقل العروض سعرًا ، إذا تبين أن لدى صاحب العرض عددًا من المشاريع ورأت اللجنة أن حجم التزاماته التعاقدية قد أصبح مرتفعًا على نحو يفوق قدراته المالية أو الفنية بما يؤثر على تنفيذه لالتزاماته التعاقدية ، وفي هذه الحالة تتفاوض مع العطاء الذي يليه وفقًا لقواعد التفاوض المحددة في هذا النظام .\n",
        "المادة الرابعة والعشرون ،إذا لم يقدم للمنافسة إلا عرض واحد ، أو قدمت عدة عروض واتضح أنها غير مطابقة للشروط والمواصفات - عدا عرض واحد - فلا يجوز قبول هذا العرض إلا إذا كانت أسعاره مماثلة للأسعار السائدة ، وكانت حاجة العمل لا تسمح بإعادة طرح المنافسة مرة أخرى ، وذلك بعد موافقة الوزير المختص أو رئيس الدائرة المستقلة .\n",
        "المادة الخامسة والعشرون،مع مراعاة ما ورد في المادتين (الحادية والعشرين) و(الرابعة والعشرين) من هذا النظام،\n",
        "لا يجوز إلغاء المنافسة إلا للمصلحة العامة ، أو لمخالفة إجراءاتها أحكام النظام ، أو لوجود أخطاء جوهرية مؤثرة في الشروط أو المواصفات ، وتكون صلاحية الإلغاء للوزير المختص أو رئيس الدائرة المستقلة .\n",
        "ترد لأصحاب العروض قيمة وثائق المنافسة في حالة الإلغاء لأسباب تعود للجهة الحكومية .\n",
        "\"\"\"\n",
        "context2=\"\"\" المادة الخامسة والثلاثون، توضح اللائحة التنفيذية لهذا النظام شروط الضمانات البنكية والمالية وأحكامها ونماذجها .\n",
        "المادة السادسة والثلاثون،يجوز للجهة الحكومية زيادة التزامات المتعاقد ضمن نطاق العقد بما لا يتجاوز (10%) عشرة في المائة من القيمة الإجمالية للعقد ، أو تخفيض هذه الالتزامات بما لا يتجاوز (20%) عشرين في المائة . وتوضح اللائحة التنفيذية الضوابط اللازمة لذلك.\n",
        "المادة السابعة والثلاثون، تدفع قيمة العقود بالريال السعودي . ويجوز أن تدفع بأي عملة أخرى بعد التنسيق مع وزارة المالية . وينص في شروط المنافسة على العملة التي يقدم بها العرض ، على أنه لا يجوز دفع قيمة العقد بأكثر من عملة واحدة .\n",
        "المادة الثامنة والثلاثون، يجوز للجهة الحكومية أن تدفع للمتعاقد معها دفعة مقدمة من استحقاقه بنسبة (5%) خمسة في المائة من القيمة الإجمالية للعقد ، بشرط ألا تتجاوز قيمة الدفعة مبلغ خمسين مليون ريال أو ما يعادلها مقابل ضمان بنكي مساوٍ لهذه القيمة ، وينص على الدفعة المقدمة ، إن وجدت ، في الشروط والمواصفات عند طرح المنافسة ، وتحسم هذه الدفعة من مستخلصات المتعاقد على أقساط ابتداءً من المستخلص الأول ، وفقًا للضوابط الموضحة في اللائحة التنفيذية .\n",
        "المادة التاسعة والثلاثون، تصرف مستحقات المقاول على دفعات طبقًا لما يتم إنجازه من عمل بموجب المستخلصات التي تعتمدها الجهة الحكومية .\n",
        "المادة الأربعون،يصرف المستخلص الأخير الذي يجب ألا يقل عن (10%) عشرة في المائة في عقود الأشغال العامة وعن (5%) خمسة في المائة في العقود الأخرى بعد تسليم الأعمال تسليمًا ابتدائيًّا ، أو توريد المشتريات .\n",
        "المادة الحادية والأربعون،يجوز للجهة الحكومية عند الحاجة وبعد الاتفاق مع وزارة المالية أن تنفذ بعض مشاريعها بحيث تسدد التكاليف على أقساط سنوية على أن تطرح مثل تلك الأعمال في منافسة عامة .\n",
        "المادة الثانية والأربعون،تكون القيمة الإجمالية للعقد شاملة ومغطية جميع تكاليف تنفيذه وفقًا لشروطه ، بما في ذلك قيمة الرسوم والضرائب التي يدفعها المتعاقد . ولا يجوز الإعفاء منها أو إعفاء أرباح المتعاقدين مع الجهة الحكومية أو دُخول موظفيهم من الضريبة أو دفعها عنهم عدا ما استثني من ذلك بموجب نص نظامي خاص .\n",
        " \"\"\"\n",
        "\n",
        "\n",
        "context3=\"\"\"  المادة السابعة عشرة ،لا يجوز الجمع بين رئاسة لجنة فحص العروض وصلاحية البت في المنافسة ، كما لا يجوز الجمع بين رئاسة لجنة فتح المظاريف ورئاسة لجنة فحص العروض أو العضوية فيهما .\n",
        "المادة الثامنة عشرة ، يجوز أن يرأس لجنة فحص العروض موظف لا تقل مرتبته عن العاشرة أو ما يعادلها ، إذا كانت مُشكَّلة في غير مقر الجهة الرئيس .  \"\"\"\n",
        "\n"
      ],
      "metadata": {
        "id": "Gb2pdhweMowE"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text =  \"ما هي الأولوية في المعاملات؟\"\n",
        "context = prep.preprocess(context1)# don't forget to preprocess the question and the context to get the optimal results\n",
        "\n",
        "result = qa_pipe_araelectra(question=text,context=context)\n",
        "QA_input = {'question':text,'context': context}\n",
        "\n",
        "res = qa_pipe_xlm(QA_input)"
      ],
      "metadata": {
        "id": "q2dx-B0K3FGo",
        "outputId": "c045b553-5c7e-4223-c001-221369d1ac81",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 346
        }
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "error",
          "ename": "IndexError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-48-5dd79c66097c>\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mQA_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m'question'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'context'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mqa_pipe_xlm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mQA_input\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0mexamples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_args_parser\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 390\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexamples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs, num_workers, batch_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1110\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreprocess_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mforward_params\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpostprocess_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1111\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mframework\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"pt\"\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mChunkPipeline\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1112\u001b[0;31m             return next(\n\u001b[0m\u001b[1;32m   1113\u001b[0m                 iter(\n\u001b[1;32m   1114\u001b[0m                     self.get_iterator(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;31m# We're out of items within a batch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 124\u001b[0;31m         \u001b[0mitem\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    125\u001b[0m         \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    126\u001b[0m         \u001b[0;31m# We now have a batch of \"inferred things\".\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/pt_utils.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    265\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_last\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 266\u001b[0;31m             \u001b[0mprocessed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minfer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    267\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloader_batch_size\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    268\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprocessed\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/base.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, model_inputs, **forward_params)\u001b[0m\n\u001b[1;32m   1024\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0minference_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m                     \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mforward_params\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m                     \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_ensure_tensor_on_device\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_outputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/pipelines/question_answering.py\u001b[0m in \u001b[0;36m_forward\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m         \u001b[0mexample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"example\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    512\u001b[0m         \u001b[0mmodel_inputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_input_names\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 513\u001b[0;31m         \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    514\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    515\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m\"start\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"start_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"end\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"end_logits\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"example\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mexample\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, start_positions, end_positions, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m   1540\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1541\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1542\u001b[0;31m         outputs = self.roberta(\n\u001b[0m\u001b[1;32m   1543\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1544\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mattention_mask\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0mhead_mask\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_head_mask\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhead_mask\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_hidden_layers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 847\u001b[0;31m         embedding_output = self.embeddings(\n\u001b[0m\u001b[1;32m    848\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m             \u001b[0mposition_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mposition_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/transformers/models/xlm_roberta/modeling_xlm_roberta.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, token_type_ids, position_ids, inputs_embeds, past_key_values_length)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m             \u001b[0minputs_embeds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mtoken_type_embeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtoken_type_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m         \u001b[0membeddings\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minputs_embeds\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtoken_type_embeddings\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/sparse.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    161\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 162\u001b[0;31m         return F.embedding(\n\u001b[0m\u001b[1;32m    163\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    164\u001b[0m             self.norm_type, self.scale_grad_by_freq, self.sparse)\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36membedding\u001b[0;34m(input, weight, padding_idx, max_norm, norm_type, scale_grad_by_freq, sparse)\u001b[0m\n\u001b[1;32m   2208\u001b[0m         \u001b[0;31m# remove once script supports set_grad_enabled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2209\u001b[0m         \u001b[0m_no_grad_embedding_renorm_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_norm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnorm_type\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2210\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membedding\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpadding_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscale_grad_by_freq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msparse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mIndexError\u001b[0m: index out of range in self"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "result"
      ],
      "metadata": {
        "id": "Hfz1B02aLkLq",
        "outputId": "64ce4fc6-e59a-4989-a5c7-23747587045a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.528122067451477,\n",
              " 'start': 1062,\n",
              " 'end': 1117,\n",
              " 'answer': 'للمصنوعات والمنتجات والخدمات الوطنية وما يعامل معاملتها'}"
            ]
          },
          "metadata": {},
          "execution_count": 49
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "res"
      ],
      "metadata": {
        "id": "2U5KwvsN8NEd",
        "outputId": "3367e397-610a-413e-fe5a-5e37fc263b38",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'score': 0.7157438397407532,\n",
              " 'start': 1061,\n",
              " 'end': 1098,\n",
              " 'answer': ' للمصنوعات والمنتجات والخدمات الوطنية'}"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}